{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport copy",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def show_v_table_small(v_table, env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+----------\"*env.reward.shape[1])\n        print(\"|\", end=\"\")\n        for j in range(env.reward.shape[1]):\n            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n        print()\n    print(\"+----------\"*env.reward.shape[1])\n\n# V table 그리기    \ndef show_v_table(v_table, env):    \n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if k==0:  \n                    print(\"                 |\",end=\"\")\n                if k==1:\n                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n                if k==2:\n                    print(\"                 |\",end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n# Q table 그리기\ndef show_q_table(q_table,env):\n    for i in range(env.reward.shape[0]):\n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if env.reward_list1[i][j]== \"wall\":\n                    print(\"                 |\",end=\"\")\n                else:\n                    if k==0:\n                        print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n                    if k==1:\n                        print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n                    if k==2:\n                        print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")   \n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n\n# 정책 policy 화살표로 그리기\ndef show_q_table_arrow(q_table,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):                           \n                if env.reward_list1[i][j]== \"wall\":\n                    print(\"                 |\",end=\"\")\n                else:\n                    if k==0:                           \n                        if np.max(q[i,j,:]) == q[i,j,0]:\n                            print(\"        ↑       |\",end=\"\")\n                        else:\n                            print(\"                 |\",end=\"\")\n                    if k==1:                    \n                        if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n                            print(\"      ←  →     |\",end=\"\")\n                        elif np.max(q[i,j,:]) == q[i,j,1]:\n                            print(\"          →     |\",end=\"\")\n                        elif np.max(q[i,j,:]) == q[i,j,3]:\n                            print(\"      ←         |\",end=\"\")\n                        else:\n                            print(\"                 |\",end=\"\")\n                    if k==2:\n                        if np.max(q[i,j,:]) == q[i,j,2]:\n                            print(\"        ↓       |\",end=\"\")\n                        else:\n                            print(\"                 |\",end=\"\")      \n           \n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")    \n    \n# 정책 policy 화살표로 그리기\ndef show_policy_small(policy,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+----------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        print(\"|\", end=\"\")\n        for j in range(env.reward.shape[1]):\n            if env.reward_list1[i][j] == \"road\":\n                if policy[i,j] == 0:\n                    print(\"   ↑     |\",end=\"\")\n                elif policy[i,j] == 1:\n                    print(\"   →     |\",end=\"\")\n                elif policy[i,j] == 2:\n                    print(\"   ↓     |\",end=\"\")\n                elif policy[i,j] == 3:\n                    print(\"   ←     |\",end=\"\")\n            else:\n                print(\"          |\",end=\"\")\n        print()\n    print(\"+----------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n# 정책 policy 화살표로 그리기\ndef show_policy(policy,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if k==0:\n                    print(\"                 |\",end=\"\")\n                if k==1:\n                    if policy[i,j] == 0:\n                        print(\"      ↑         |\",end=\"\")\n                    elif policy[i,j] == 1:\n                        print(\"      →         |\",end=\"\")\n                    elif policy[i,j] == 2:\n                        print(\"      ↓         |\",end=\"\")\n                    elif policy[i,j] == 3:\n                        print(\"      ←         |\",end=\"\")\n                if k==2:\n                    print(\"                 |\",end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Agent():\n    \n    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n    \n    # 2. 각 행동별 선택확률\n    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n    \n    # 3. 에이전트의 초기 위치 저장\n    def __init__(self):\n        self.pos = (0,0)\n    \n    # 4. 에이전트의 위치 저장\n    def set_pos(self,position):\n        self.pos = position\n        return self.pos\n    \n    # 5. 에이전트의 위치 불러오기\n    def get_pos(self):\n        return self.pos",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Environment():\n    \n    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n    cliff = -4\n    wall = -3\n    road = 0\n    goal = 3\n    \n    # 2. 목적지 좌표 설정\n    goal_position = [5,5]\n    \n    # 3. 보상 리스트 숫자\n    reward_list = [[road,road,wall,road,wall,wall],\n                   [wall,road,wall,road,road,road],\n                   [road,road,wall,wall,road,road],\n                   [road,wall,road,road,road,road],\n                   [road,wall,road,road,road,road],\n                   [road,road,road,road,wall,goal]]\n    \n    # 4. 보상 리스트 문자\n    reward_list1 = [[\"road\",\"road\",\"wall\",\"road\",\"wall\",\"wall\"],\n                    [\"wall\",\"road\",\"wall\",\"road\",\"road\",\"road\"],\n                    [\"road\",\"road\",\"wall\",\"wall\",\"road\",\"road\"],\n                    [\"road\",\"wall\",\"road\",\"road\",\"road\",\"road\"],\n                    [\"road\",\"wall\",\"road\",\"road\",\"road\",\"road\"],\n                    [\"road\",\"road\",\"road\",\"road\",\"wall\",\"goal\"]]\n    \n    # 5. 보상 리스트를 array로 설정\n    def __init__(self):\n        self.reward = np.asarray(self.reward_list)    \n\n    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n    def move(self, agent, action):\n        \n        done = False\n        \n        # 6.1 행동에 따른 좌표 구하기\n        new_pos = agent.pos + agent.action[action]\n        \n        # 6.2 현재좌표가 목적지 인지확인\n        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n            reward = self.goal\n            observation = agent.set_pos(agent.pos)\n            done = True\n        # 6.3 현재좌표가 벽 인지확인\n        elif self.reward_list1[agent.pos[0]][agent.pos[1]] == \"wall\":\n            reward = self.wall\n            observation = agent.set_pos(agent.pos)\n            done = True \n        # 6.3 이동 후 좌표가 미로 밖인 확인    \n        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n            reward = self.cliff\n            observation = agent.set_pos(agent.pos)\n            done = True\n        # 6.4 이동 후 좌표가 길이라면\n        else:\n            observation = agent.set_pos(new_pos)\n            reward = self.reward[observation[0],observation[1]]\n            \n        return observation, reward, done",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 행동 가치 함수\ndef action_value_function(env, agent, act, G, max_step, now_step):   \n    \n    # 1. 감가율 설정\n    gamma = 0.9\n    \n    # 2. 현재 위치가 목적지인지 확인\n    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n        return env.goal\n\n    # 3. 마지막 상태는 보상만 계산\n    if (max_step == now_step):\n        observation, reward, done = env.move(agent, act)\n        G += agent.select_action_pr[act]*reward\n        return G\n    \n    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n    else:\n        # 4.1현재 위치 저장\n        pos1 = agent.get_pos()\n        observation, reward, done = env.move(agent, act)\n        G += agent.select_action_pr[act] * reward\n        \n        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n        if done == True:            \n            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1] or env.reward_list1[agent.pos[0]][agent.pos[1]] == \"wall\":\n                agent.set_pos(pos1) \n            \n        # 4.3 현재 위치를 다시 저장\n        pos1 = agent.get_pos()\n        \n        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n        for i in range(len(agent.action)):\n            agent.set_pos(pos1)\n            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n            G += agent.select_action_pr[i] * gamma * next_v\n        return G",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# 재귀적으로 행동의 가치를 계산\n\n# 1. 환경 초기화\nenv = Environment()\n\n# 2. 에이전트 초기화\nagent = Agent()\nnp.random.seed(0)\n\n# 3. 현재부터 max_step 까지 계산\nmax_step_number = 6\n\n# 4. 모든 상태에 대해\n # for max_step in range(max_step_number):\nmax_step=max_step_number\n    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\nprint(\"max_step = {}\".format(max_step))\nq_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        # 4.2 모든 행동에 대해\n        for action in range(len(agent.action)):\n            # 4.2.1 에이전트의 위치를 초기화\n            agent.set_pos([i,j])\n            # 4.2.2 현재 위치에서 행동 가치를 계산\n            q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n\nq = np.round(q_table,2)\nprint(\"Q - table\")\nshow_q_table(q, env)\nprint(\"High actions Arrow\")\nshow_q_table_arrow(q,env)\nprint()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "max_step = 6\nQ - table\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|     -3.83       |     -3.54       |                 |     -3.83       |                 |                 |\n| -3.83     -2.54 | -2.83     -3.91 |                 | -3.91     -3.91 |                 |                 |\n|     -3.91       |     -2.55       |                 |     -2.50       |                 |                 |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |     -2.54       |                 |     -2.83       |     -3.91       |     -3.91       |\n|                 | -3.91     -3.91 |                 | -3.91     -1.94 | -2.50     -2.00 | -1.94     -3.00 |\n|                 |     -2.53       |                 |     -3.91       |     -1.54       |     -1.25       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|     -3.91       |     -2.55       |                 |                 |     -1.94       |     -2.00       |\n| -3.41     -2.53 | -2.41     -3.91 |                 |                 | -3.91     -1.25 | -1.54     -2.25 |\n|     -2.38       |     -3.91       |                 |                 |     -0.88       |     -0.63       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|     -2.41       |                 |     -3.91       |     -3.91       |     -1.54       |     -1.25       |\n| -3.38     -3.91 |                 | -3.91     -1.57 | -2.19     -0.88 | -1.57     -0.63 | -0.88     -1.63 |\n|     -2.33       |                 |     -1.71       |     -1.15       |     -1.05       |      0.34       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|     -2.38       |                 |     -2.19       |     -1.57       |     -0.88       |     -0.63       |\n| -3.33     -3.91 |                 | -3.91     -1.15 | -1.71     -1.05 | -1.15      0.34 | -1.05     -0.66 |\n|     -2.14       |                 |     -1.61       |     -1.88       |     -3.91       |      3.45       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|     -2.33       |     -3.91       |     -1.71       |     -1.15       |                 |      3.00       |\n| -3.14     -2.14 | -2.14     -1.61 | -2.14     -1.88 | -1.61     -3.91 |                 |  3.00      3.00 |\n|     -3.14       |     -3.14       |     -2.61       |     -2.88       |                 |      3.00       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\nHigh actions Arrow\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |                 |                 |                 |                 |\n|          →     |                 |                 |                 |                 |                 |\n|                 |        ↓       |                 |        ↓       |                 |                 |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |                 |                 |                 |                 |\n|                 |                 |                 |          →     |                 |                 |\n|                 |        ↓       |                 |                 |        ↓       |        ↓       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |                 |                 |                 |                 |\n|                 |      ←         |                 |                 |                 |                 |\n|        ↓       |                 |                 |                 |        ↓       |        ↓       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |                 |                 |                 |                 |\n|                 |                 |          →     |          →     |          →     |                 |\n|        ↓       |                 |                 |                 |                 |        ↓       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |                 |                 |                 |                 |\n|                 |                 |          →     |          →     |          →     |                 |\n|        ↓       |                 |                 |                 |                 |        ↓       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n|                 |                 |        ↑       |        ↑       |                 |        ↑       |\n|          →     |          →     |                 |                 |                 |      ←  →     |\n|                 |                 |                 |                 |                 |        ↓       |\n+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}